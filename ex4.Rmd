---
title: "Statistics for e-Science"
subtitle: "Week 4 exercises"
author: "Tycho Canter, Gowthami Rajukkannu and Antonio Ortega"
date: "December 19, 2016"
output: ioslides_presentation
runtime: shiny
---

```{r setup, include = FALSE, tidy = T}
knitr::opts_chunk$set(echo = FALSE, root.dir = '/home/antortjim/MEGA/Master/STATS/week4/exercises')
```

```{r, echo = F, message = F}
library("ggfortify")
library("magrittr")

# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

```

## Exercise 4.4

**The likelihood ratio test and the two-sample t-test**

Checking the potential of the likelihood ratio and the two-sample t tests.

- Simulate data coming from similar, but *not identical* normal distributions

- Run tests to evalute the significance of the difference of means

- Conclude whether distributions can be approximated or not and tests yield

## 4.4.1 - Simulate data

Assume: 

$X \sim N(\mu = 2, \sigma = 4)$

$Y \sim N(\mu = 2.5, \sigma = 4)$



```{r, echo = T}
x <- rnorm(20, 2, 4)
y <- rnorm(40, 2.5, 4)
xy <- c(x, y)
```
- x consists of a sample of X with size 20

- y consists of a sample of Y with size 40

- xy results from merging both samples into a single one


```{r}
x.mean <- mean(x)
y.mean <- mean(y)
```


## Theory and practice
```{r, message = F}
p <- ggdistribution(dnorm, x = seq(-10, 14, 0.1), mean = 2, sd = 4, fill = "blue", alpha = 0.2) +
  geom_vline(xintercept = 2, linetype = "longdash") +
  scale_x_continuous(breaks = seq(-10, 14, 2))

p <- ggdistribution(dnorm, x = seq(-10, 14, 0.1), p = p, mean = 2.5, sd = 4, fill = "red", alpha = 0.2) +
  geom_vline(xintercept = 2.5, linetype = "longdash") +
  ggtitle(label = "Theoretical distributions")

q <- autoplot(density(x), fill = "blue")
q <- autoplot(density(y), p = q, fill = "red") +
  ggtitle("Simulated data") +
  scale_x_continuous(breaks = seq(-10, 14, 2))

multiplot(p, q, cols = 2)
```

## Models

* **Full model: distinguish between X and Y**

x and y come from different distributions (different means)

Parameter space: $\Theta \rightarrow \{ \mu_X, \mu_Y, \sigma \}$


* **Nested model: no distinction between X and Y**

Assume x and y came from the same distribution

Parameter space: $\Theta_0 \rightarrow \{ \mu, \sigma \}$
$( \mu_X = \mu_Y )$


**The nested model is a simplification of the full model. Assumes we can consider both means similar for simplicity**

## 4.4.2 - Perform MLE and compute ML values

*MLE = Maximum Likelihood Estimation*

We want to minimise the -log(likelihood) value for each model using the simulated data

Minimise: $-\sum { \log { f_N\left( x_{ i } \right)  } }$

```{r, echo = T}
mllk.normal <- function(params, data) {
  -(sum(log(dnorm(data, mean = params, sd = 4))))
}
```

## R implementation

Minimise (optimise), i.e get the parameters that minimise:

Full model                                                                                                                                      | Nested model 
-------------                                                                                                                                   | ------------- 
$- \left( \sum_{ i = 1 }^{ 20 } { \log { f_X\left( x_{ i } \right)  } } + \sum_{ i = 1}^{40} { \log { f_Y\left( y_{ i } \right)  } } \right)$  | $- \sum_{ i = 1 }^{ 60 } { \log { f_XY\left( xy_{ i } \right) } }$        




```{r, echo = T}
# Full model
x.params <- optimize(mllk.normal, interval = c(-10, 10), data = x)
y.params <- optimize(mllk.normal, interval = c(-10, 10), data = y)

# Nested model
xy.params <- optimize(mllk.normal, interval = c(-10, 10), data = xy)

```

## Compute likelihood using optimized parameters

1. Compute probability of each value according to model with ML parameters
2. Multiply all probabilities to obtain the likelihood of the parameters

```{r, echo = T}
# Full model
full.model.l <- c(dnorm(x, mean = x.params$minimum, sd = 4),
                  dnorm(y, mean = y.params$minimum, sd = 4)) %>%
  prod

# Nested model
nested.model.l <- dnorm(xy, mean = xy.params$minimum, sd = 4) %>%
  prod
```

## 4.4.3 - Compute the LRT statistic and the p-value

Compute the quotient of likelihoods between the nested model and the full model

*This quotient will ALWAYS be* $\le 1$

```{r, echo = T}
q <- nested.model.l / full.model.l
```

A statistic based on q is said to follow a $\chi^2$ distribution if the nested model captures the same info as the full one

$-2\log{\left( q \right)} \sim \chi^{2}_{n}$

```{r}
statistic <- -2 * log(q)
```

## P-value (LRT)

$H_0$: The nested model is sufficient

* The p-value will give us the probability of observing a statistic more extreme than the observed

* If it is low, we reject the null hypothesis

```{r, echo = T}
lrt.pval <- 1 - pchisq(statistic, df = 1)
lrt.pval
```

## 4.4.4 P-value (T-test)

$H_0$: The nested model is sufficient

* We can also use the student's T test

* All distributions considered have the same variance (homoscedastic)

```{r, echo = T}
ttest.pval <- t.test(x, y,
                     var.equal = TRUE,
                     alternative = "two.sided")$p.value

ttest.pval
```

## Compare p-values

```{r}
ggplot(data = data.frame(test = as.factor(c("Likelihood ratio", "Student's T")), p_value = c(lrt.pval, ttest.pval))) +
  geom_bar(aes(x = test, y = p_value, fill = test), stat = "identity") +
  scale_y_continuous(limits = 0:1)
```


## 4.5 - Simulations on Confidence Intervals

Study the behaviour of Confidence Intervals according to:

* Sample size

* $\alpha$, the probability of leaving the actual mean out of the interval

## 4.5.1 - Set n = 20, μ = 2, σ = 3, N = 10000


```{r, echo = T}
n <- 20
mu <- 2
sigma <- 3
N <- 1000
```

## 4.5.2 - Simulate an observation x of length n following N (μ, σ) with mean μ and standard deviation σ.

```{r, echo = T}
x <- rnorm(n, mu, sigma)
```

## 4.5.3 - Compute the 95%-confidence interval for μ using data x.

Process each sample 

```{r, echo = T}
# Fix p to 0.95
p <- 0.95
# Extract percentil 95
z <- qnorm(p + (1 - p) / 2, mean = mu, sd = sigma)

# Compute mean and sem of sample
average <- mean(x)
sem <- sd(x)/sqrt(n)
```

## 4.5.3 - Compute the 95%-confidence interval for μ using data x.

Build the confidence interval

```{r, echo = T}
inferior <- average - z * sem
superior <- average + z * sem
```

## 4.5.4 - Check if the confidence interval contains μ and output a True or False.

```{r, echo = T}
result <- mu > inferior & mu < superior
```

## 4.5.5 - Repeat steps 2 - 4 for N times. How many repetitions give a True?

* Put all the code in 4.5.2 - 4.5.4 in a foor loop running N times

* Store mean and sem of each sample

## Visualization

```{r, echo = F}
library("shiny")
shinyAppDir(appDir = "ex4_5/")
```


## 4.7 - Can the exponential distribution provide enough detail to fit our data?

Compare the fit offered by the gamma and exponential distributions

## Download the data

```{r, echo = T}
neuron <- read.table("http://math.ku.dk/~tfb525/teaching/statbe/neuronspikes.txt", col.names = "time")
```

## Formulate expression to optimise

```{r, echo = T}
mllk.gamma <- function(params, data) {
  -(dgamma(data, shape = params[1], rate = params[2]) %>% log %>% sum)
}

mllk.exponential <- function(rate, data) {
  -(dexp(data, rate) %>% log %>% sum)
}
```

## Optimise

```{r, echo = T, message = F, warning= F}
exp.optim <- optimize(mllk.exponential, interval = c(-10, 10), data = neuron$time)
exp.optim <- exp.optim$minimum

gamma.optim <- optim(c(1, 1), mllk.gamma, data = neuron$time)
```

## Compute likelihoods and statistic

```{r, echo = T}
MLL1 <- mllk.exponential(exp.optim, neuron$time)
MLL2 <- mllk.gamma(gamma.optim$par, neuron$time)

statistic <- 2 * (MLL1 - MLL2)
```

```{r, eval = T}
MLL1
MLL2
statistic
```


## Compute p-value

```{r, echo = T}
p.value <- 1 - pchisq(statistic, df = 1)
```

```{r, eval = T}
p.value
```

The obtained p-value is very low, thus we discard the null hypothesis

**The nested model is not sufficient**.

**The gamma distribution provides a better fit to our data**.

## Visualization

```{r, echo = F}
library("shiny")
shinyAppDir(appDir = "ex4_7/")
```

